{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFER LEARNING ON TRANSFORMER TO GIVE INFO ABOUT THE COSTA RICAN DISH\n",
    "\n",
    "## Fine-Tuning GPT-2 for Recipe Generation: Training and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "# Set the device to GPU or Apple M1 (MPS) if available, otherwise CPU\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Function to split the dataset\n",
    "def split_dataset(filename, train_ratio=0.8):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().split('---end-of-recipe---')\n",
    "\n",
    "    random.shuffle(content)\n",
    "    train_size = int(len(content) * train_ratio)\n",
    "    train_data = content[:train_size]\n",
    "    validation_data = content[train_size:]\n",
    "\n",
    "    return train_data, validation_data\n",
    "\n",
    "# Split the dataset and save it in different files\n",
    "train_data, validation_data = split_dataset('../dataset-transformers/dishes_train_v1.txt')\n",
    "train_filename = 'train_dataset_v1.txt'\n",
    "validation_filename = 'validation_dataset_v1.txt'\n",
    "\n",
    "with open(train_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('---end-of-recipe---'.join(train_data))\n",
    "\n",
    "with open(validation_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('---end-of-recipe---'.join(validation_data))\n",
    "\n",
    "# Load the GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Ensure the tokenizer uses the correct pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Custom Dataset class for recipes\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, tokenizer, filename, block_size=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "\n",
    "        # Read and split the dataset file\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            recipes = f.read().split('---end-of-recipe---')\n",
    "\n",
    "        # Encode recipes and add to examples\n",
    "        for recipe in recipes:\n",
    "            if recipe.strip() == \"\":\n",
    "                continue\n",
    "\n",
    "            tokens = tokenizer.encode_plus(recipe, \n",
    "                                            add_special_tokens=True, \n",
    "                                            max_length=block_size, \n",
    "                                            padding='max_length', \n",
    "                                            truncation=True, \n",
    "                                            return_tensors='pt')\n",
    "\n",
    "            self.examples.append(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Get individual item from dataset\n",
    "        input_ids = self.examples[i]['input_ids'][0]\n",
    "        attention_mask = self.examples[i]['attention_mask'][0]\n",
    "        labels = input_ids.clone() # Labels for language modeling\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = RecipeDataset(tokenizer, '../dataset-transformers/dishes_train_v1.txt')\n",
    "\n",
    "# Create a DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "# Define training arguments for fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt2_finetuned_recipes_v1',\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=2,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    ")\n",
    "\n",
    "# Load validation dataset\n",
    "validation_dataset = RecipeDataset(tokenizer, validation_filename)\n",
    "\n",
    "# Initialize trainer for model fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=validation_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Print log history\n",
    "print(trainer.state.log_history)\n",
    "\n",
    "# Extract and print training loss\n",
    "training_loss_run1 = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
    "validation_loss_v1 = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
    "print(training_loss_run1)\n",
    "print(validation_loss_v1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
